{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c5fb0b9e-f9cd-404f-bd8d-0273e94ac1fe",
   "metadata": {},
   "source": [
    "# RAG Example Using NVIDIA API Catalog and LangChain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2969cdab-82fc-4ce5-bde1-b4f629691f27",
   "metadata": {},
   "source": [
    "This notebook introduces how to use LangChain to interact with NVIDIA hosted NIM microservices like chat, embedding, and reranking models to build a simple retrieval-augmented generation (RAG) application."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4253bd0-4313-4056-95f5-899a180879c2",
   "metadata": {},
   "source": [
    "## Terminology"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "5a084a00-b65d-483a-a7c6-b4c12e4272dd",
   "metadata": {},
   "source": [
    "#### RAG\n",
    "\n",
    "- RAG is a technique for augmenting LLM knowledge with additional data.\n",
    "- LLMs can reason about wide-ranging topics, but their knowledge is limited to the public data up to a specific point in time that they were trained on.\n",
    "- If you want to build AI applications that can reason about private data or data introduced after a model's cutoff date, you need to augment the knowledge of the model with the specific information it needs.\n",
    "- The process of bringing the appropriate information and inserting it into the model prompt is known as retrieval augmented generation (RAG).\n",
    "\n",
    "The preceding summary of RAG originates in the LangChain v0.2 tutorial [Build a RAG App](https://python.langchain.com/v0.2/docs/tutorials/rag/) tutorial in the LangChain v0.2 documentation.\n",
    "\n",
    "#### NIM\n",
    "\n",
    "- [NIM microservices](https://developer.nvidia.com/blog/nvidia-nim-offers-optimized-inference-microservices-for-deploying-ai-models-at-scale/) are containerized microservices that simplify the deployment of generative AI models like LLMs and are optimized to run on NVIDIA GPUs. \n",
    "- NIM microservices support models across domains like chat, embedding, reranking, and more from both the community and NVIDIA.\n",
    "\n",
    "#### NVIDIA API Catalog\n",
    "\n",
    "- [NVIDIA API Catalog](https://build.nvidia.com/explore/discover) is a hosted platform for accessing a wide range of microservices online.\n",
    "- You can test models on the catalog and then export them with an NVIDIA AI Enterprise license for on-premises or cloud deployment\n",
    "\n",
    "#### langchain-nvidia-ai-endpoints\n",
    "\n",
    "- The [`langchain-nvidia-ai-endpoints`](https://pypi.org/project/langchain-nvidia-ai-endpoints/) Python package contains LangChain integrations for building applications that communicate with NVIDIA NIM microservices."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca300278-5ff4-47c4-ab70-c6584ef73c9f",
   "metadata": {},
   "source": [
    "## Installation and Requirements\n",
    "\n",
    "Create a Python environment (preferably with Conda) using Python version 3.10.14. \n",
    "To install Jupyter Lab, refer to the [installation](https://jupyter.org/install) page."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5de8e53a-7940-4e72-87f3-e1c014128806",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Requirements\n",
    "!pip install langchain==0.2.5\n",
    "!pip install langchain_community==0.2.5\n",
    "!pip install faiss-cpu==1.8.0 # replace with faiss-gpu if you are using GPU\n",
    "!pip install langchain-nvidia-ai-endpoints==0.1.2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b7a52a0-7e5e-4064-9665-cb947d600f84",
   "metadata": {},
   "source": [
    "## Getting Started!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04495732-c2db-4c97-91d0-96708814334d",
   "metadata": {},
   "source": [
    "To get started you need an `NVIDIA_API_KEY` to use the NVIDIA API Catalog:\n",
    "\n",
    "1) Create a free account with [NVIDIA](https://build.nvidia.com/explore/discover).\n",
    "2) Click on your model of choice.\n",
    "3) Under Input select the Python tab, and click **Get API Key** and then click **Generate Key**.\n",
    "4) Copy and save the generated key as NVIDIA_API_KEY. From there, you should have access to the endpoints."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bbb51115-79f8-48c3-b3ee-d434916945f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import getpass\n",
    "import os\n",
    "\n",
    "if not os.environ.get(\"NVIDIA_API_KEY\", \"\").startswith(\"nvapi-\"):\n",
    "    nvidia_api_key = getpass.getpass(\"Enter your NVIDIA API key: \")\n",
    "    assert nvidia_api_key.startswith(\"nvapi-\"), f\"{nvidia_api_key[:5]}... is not a valid key\"\n",
    "    os.environ[\"NVIDIA_API_KEY\"] = nvidia_api_key"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25656ab5-0046-4e27-be65-b3d3d547b4c6",
   "metadata": {},
   "source": [
    "## RAG Example using LLM & Embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54e86bc0-e9c5-4a2b-be0e-7fca0331e886",
   "metadata": {},
   "source": [
    "### 1) Initialize the LLM\n",
    "\n",
    "The ChatNVIDIA class is part of LangChain's integration (langchain_nvidia_ai_endpoints) with NVIDIA NIM microservices. \n",
    "It allows access to NVIDIA NIM for chat applications, connecting to hosted or locally-deployed microservices.\n",
    "\n",
    "Here we will use **mixtral-8x7b-instruct-v0.1** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "88c2fafe-5ded-4238-82de-f094232bf6fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_nvidia_ai_endpoints import ChatNVIDIA\n",
    "\n",
    "llm = ChatNVIDIA(model=\"mistralai/mixtral-8x7b-instruct-v0.1\", max_tokens=1024)\n",
    "\n",
    "# Here we are using mixtral-8x7b-instruct-v0.1 model\n",
    "# But you are free to choose any model hosted at Nvidia API Catalog\n",
    "# Uncomment the below code to list the availabe models\n",
    "# ChatNVIDIA.get_available_models()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35cc87a6-2f83-4652-95f1-cf349db8bad6",
   "metadata": {},
   "source": [
    "### 2) Intiatlize the embedding\n",
    "NVIDIAEmbeddings is a client to NVIDIA embeddings models that provides access to a NVIDIA NIM for embedding. It can connect to a hosted NIM or a local NIM using a base URL\n",
    "\n",
    "We selected **NV-Embed-QA** as the embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d88f7838-b9f9-4fc5-8779-84df6cb26017",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_nvidia_ai_endpoints import NVIDIAEmbeddings\n",
    "\n",
    "embedder = NVIDIAEmbeddings(model=\"NV-Embed-QA\", truncate=\"END\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9862f2e-5055-4fe4-818d-708091243d74",
   "metadata": {},
   "source": [
    "### 3) Obtain some toy text dataset\n",
    "Here we are loading a toy data from a text documents and in real-time data can be loaded from various sources. \n",
    "Read [here](https://python.langchain.com/v0.2/docs/tutorials/rag/#go-deeper) for loading data from different sources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9c615b9c-527e-4e3b-86b7-49ef258e2d57",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# For this example we load a toy data set (it's a simple text file with some information about Sweden)\n",
    "TOY_DATA_PATH = \"./data/\"\n",
    "# We read in the text data and prepare them into vectorstore\n",
    "ps = os.listdir(TOY_DATA_PATH)\n",
    "data = []\n",
    "sources = []\n",
    "for p in ps:\n",
    "    if p.endswith('.txt'):\n",
    "        path2file = TOY_DATA_PATH + p\n",
    "        with open(path2file, encoding=\"utf-8\") as f:\n",
    "            lines = f.readlines()\n",
    "            for line in lines:\n",
    "                if len(line) >= 1:\n",
    "                    data.append(line)\n",
    "                    sources.append(path2file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d0a1447d-444a-4ae9-9484-4546424c047d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(400,\n",
       " 230,\n",
       " 'Sweden, formally the Kingdom of Sweden, is a Nordic country located on the Scandinavian Peninsula in Northern Europe. It borders Norway to the west and north, Finland to the east, and is connected to Denmark in the southwest by a bridge–tunnel across the Öresund. At 447,425 square kilometres (172,752 sq mi), Sweden is the largest Nordic country, the third-largest country in the European Union, and the fifth-largest country in Europe. The capital and largest city is Stockholm. Sweden has a total population of 10.5 million, and a low population density of 25.5 inhabitants per square kilometre (66/sq mi), with around 87% of Swedes residing in urban areas, which cover 1.5% of the entire land area, in the central and southern half of the country.\\n')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Do some basic cleaning and remove empty lines\n",
    "documents=[d for d in data if d != '\\n']\n",
    "len(data), len(documents), data[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "851b16b3-43ac-4269-9f37-05a33efe24fb",
   "metadata": {},
   "source": [
    "### 4) Process the documents into vectorstore and save it to disk\n",
    "\n",
    "Real world documents can be very long, this makes it hard to fit in the context window of many models. Even for those models that could fit the full post in their context window, models can struggle to find information in very long inputs.\n",
    "\n",
    "To handle this we’ll split the Document into chunks for embedding and vector storage. More on text splitting [here](https://python.langchain.com/v0.2/docs/concepts/#text-splitters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "804c85f6-181b-4291-a685-d6b378015544",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we create a faiss vector store from the documents and save it to disk.\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "\n",
    "text_splitter = CharacterTextSplitter(chunk_size=400, separator=\" \", chunk_overlap=80)\n",
    "docs = []\n",
    "metadatas = []\n",
    "\n",
    "for i, d in enumerate(documents):\n",
    "    splits = text_splitter.split_text(d)\n",
    "    docs.extend(splits)\n",
    "    metadatas.extend([{\"source\": sources[i]}] * len(splits))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f867df18-11c8-45ea-b81c-1603459431f9",
   "metadata": {},
   "source": [
    "To enable runtime search, we index text chunks by embedding each document split and storing these embeddings in a vector database. Later to search, we embed the query and perform a similarity search to find the stored splits with embeddings most similar to the query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2d7b2fbd-8cb1-4d68-9659-2426b9ecffe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# you will only need to do this once, later on we will restore the already saved vectorstore\n",
    "# store = FAISS.from_texts(docs, embedder , metadatas=metadatas)\n",
    "VECTOR_STORE = './data/nv_embedding'\n",
    "# store.save_local(VECTOR_STORE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fe85dad-12bb-47d2-a407-9b89b5270d4e",
   "metadata": {},
   "source": [
    "### 5) Read the previously processed & saved vectore store back"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5de3e07d-5fbe-4fe7-8f23-ed0b082f2413",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the FAISS vectorestore back.\n",
    "store = FAISS.load_local(VECTOR_STORE, embedder, allow_dangerous_deserialization=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a41ff63-6adc-4055-8bc4-e7ecaad0fb4d",
   "metadata": {},
   "source": [
    "### 6) Wrap the restored vectorsore into a retriever and ask our question "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5aa362c9-48ab-4646-bc29-bc2aca92505d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'根据提供的上下文，我可以告诉你瑞典有着丰富的文化。然而，由于提供的详细信息很有限，我只能简要介绍一下瑞典文化的一些方面。\\n\\n请注意，以下内容并不是详尽的瑞典文化描述，仅供参考。\\n\\n瑞典的文化被 Europen Union 列为非物质文化遗产，这说明瑞典文化的 immense value。瑞典有着独特的文化传统，包括面具和木偶戏、诗歌和民歌，以及传统节日和セリモン尼（Samernas högtider）。\\n\\n另外，影响瑞典文化的因素还有其地理位置，以及历史上与挪威、丹麦、芬兰和其他北欧国家的联系。这都使得瑞典的文化日益发达和多元化。\\n\\n请注意，这里的信息每个细节都源自提供的文本，文本并没有提供详细的信息。如果你需要更多关于瑞典文化的信息，建议去查阅更多的资料。'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 创建检索器    \n",
    "retriever = store.as_retriever()\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"Answer solely based on the following context:\\n<Documents>\\n{context}\\n</Documents>\",\n",
    "        ),\n",
    "        (\"user\", \"{question}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Langchain's LCEL(LangChain Expression Language) Runnable protocol is used to define the chain\n",
    "# LCEL allows pipe together components and functions\n",
    "chain = (\n",
    "    {\"context\": retriever, \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "# chain.invoke(\"Tell me about Sweden.\")\n",
    "# chain.invoke(\"请向我介绍瑞典。\")\n",
    "# chain.invoke(\"请使用中文向我介绍瑞典的教育。\")\n",
    "chain.invoke(\"请使用中文向我介绍瑞典的文化。\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0e5c0fd8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The output should be formatted as a JSON instance that conforms to the JSON schema below.\n",
      "\n",
      "As an example, for the schema {\"properties\": {\"foo\": {\"title\": \"Foo\", \"description\": \"a list of strings\", \"type\": \"array\", \"items\": {\"type\": \"string\"}}}, \"required\": [\"foo\"]}\n",
      "the object {\"foo\": [\"bar\", \"baz\"]} is a well-formatted instance of the schema. The object {\"properties\": {\"foo\": [\"bar\", \"baz\"]}} is not well-formatted.\n",
      "\n",
      "Here is the output schema:\n",
      "```\n",
      "{\"properties\": {\"answer\": {\"description\": \"The main answer to the user's query\", \"title\": \"Answer\", \"type\": \"string\"}, \"source\": {\"description\": \"The source of the answer, such as document ID or URL\", \"title\": \"Source\", \"type\": \"string\"}}, \"required\": [\"answer\", \"source\"]}\n",
      "```\n",
      "Yes\n"
     ]
    },
    {
     "ename": "OutputParserException",
     "evalue": "Invalid json output: {\"Tell me about Sweden.\"}",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mJSONDecodeError\u001b[0m                           Traceback (most recent call last)",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain_core/output_parsers/json.py:84\u001b[0m, in \u001b[0;36mJsonOutputParser.parse_result\u001b[0;34m(self, result, partial)\u001b[0m\n\u001b[1;32m     83\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 84\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mparse_json_markdown\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     85\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m JSONDecodeError \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain_core/utils/json.py:147\u001b[0m, in \u001b[0;36mparse_json_markdown\u001b[0;34m(json_string, parser)\u001b[0m\n\u001b[1;32m    146\u001b[0m         json_str \u001b[38;5;241m=\u001b[39m match\u001b[38;5;241m.\u001b[39mgroup(\u001b[38;5;241m2\u001b[39m)\n\u001b[0;32m--> 147\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_parse_json\u001b[49m\u001b[43m(\u001b[49m\u001b[43mjson_str\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparser\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparser\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain_core/utils/json.py:163\u001b[0m, in \u001b[0;36m_parse_json\u001b[0;34m(json_str, parser)\u001b[0m\n\u001b[1;32m    162\u001b[0m \u001b[38;5;66;03m# Parse the JSON string into a Python dictionary\u001b[39;00m\n\u001b[0;32m--> 163\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mparser\u001b[49m\u001b[43m(\u001b[49m\u001b[43mjson_str\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain_core/utils/json.py:118\u001b[0m, in \u001b[0;36mparse_partial_json\u001b[0;34m(s, strict)\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[38;5;66;03m# If we got here, we ran out of characters to remove\u001b[39;00m\n\u001b[1;32m    116\u001b[0m \u001b[38;5;66;03m# and still couldn't parse the string as JSON, so return the parse error\u001b[39;00m\n\u001b[1;32m    117\u001b[0m \u001b[38;5;66;03m# for the original string.\u001b[39;00m\n\u001b[0;32m--> 118\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mjson\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloads\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstrict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstrict\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/lib/python3.10/json/__init__.py:359\u001b[0m, in \u001b[0;36mloads\u001b[0;34m(s, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)\u001b[0m\n\u001b[1;32m    358\u001b[0m     kw[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mparse_constant\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m parse_constant\n\u001b[0;32m--> 359\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/lib/python3.10/json/decoder.py:337\u001b[0m, in \u001b[0;36mJSONDecoder.decode\u001b[0;34m(self, s, _w)\u001b[0m\n\u001b[1;32m    333\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Return the Python representation of ``s`` (a ``str`` instance\u001b[39;00m\n\u001b[1;32m    334\u001b[0m \u001b[38;5;124;03mcontaining a JSON document).\u001b[39;00m\n\u001b[1;32m    335\u001b[0m \n\u001b[1;32m    336\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m--> 337\u001b[0m obj, end \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mraw_decode\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43midx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_w\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mend\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    338\u001b[0m end \u001b[38;5;241m=\u001b[39m _w(s, end)\u001b[38;5;241m.\u001b[39mend()\n",
      "File \u001b[0;32m/usr/lib/python3.10/json/decoder.py:353\u001b[0m, in \u001b[0;36mJSONDecoder.raw_decode\u001b[0;34m(self, s, idx)\u001b[0m\n\u001b[1;32m    352\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 353\u001b[0m     obj, end \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscan_once\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43midx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    354\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "\u001b[0;31mJSONDecodeError\u001b[0m: Expecting ':' delimiter: line 1 column 25 (char 24)",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mOutputParserException\u001b[0m                     Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[21], line 34\u001b[0m\n\u001b[1;32m     31\u001b[0m parsed_output \u001b[38;5;241m=\u001b[39m output_parser\u001b[38;5;241m.\u001b[39mparse(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m{\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124manswer\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m: \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYes\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msource\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m: \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mexample.com\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m}\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     32\u001b[0m \u001b[38;5;28mprint\u001b[39m(parsed_output\u001b[38;5;241m.\u001b[39manswer)\n\u001b[0;32m---> 34\u001b[0m parsed_output \u001b[38;5;241m=\u001b[39m \u001b[43moutput_parser\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparse_with_prompt\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m{\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mTell me about Sweden.\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m}\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     35\u001b[0m \u001b[38;5;28mprint\u001b[39m(parsed_output\u001b[38;5;241m.\u001b[39manswer)\n\u001b[1;32m     38\u001b[0m \u001b[38;5;66;03m# # Langchain's LCEL(LangChain Expression Language) Runnable protocol is used to define the chain\u001b[39;00m\n\u001b[1;32m     39\u001b[0m \u001b[38;5;66;03m# # LCEL allows pipe together components and functions\u001b[39;00m\n\u001b[1;32m     40\u001b[0m \u001b[38;5;66;03m# chain = (\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[38;5;66;03m#     result = ResponseFormat(answer=\"无法生成答案\", source=\"未知\")\u001b[39;00m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;66;03m#     print(result)\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain_core/output_parsers/base.py:295\u001b[0m, in \u001b[0;36mBaseOutputParser.parse_with_prompt\u001b[0;34m(self, completion, prompt)\u001b[0m\n\u001b[1;32m    281\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mparse_with_prompt\u001b[39m(\u001b[38;5;28mself\u001b[39m, completion: \u001b[38;5;28mstr\u001b[39m, prompt: PromptValue) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m    282\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Parse the output of an LLM call with the input prompt for context.\u001b[39;00m\n\u001b[1;32m    283\u001b[0m \n\u001b[1;32m    284\u001b[0m \u001b[38;5;124;03m    The prompt is largely provided in the event the OutputParser wants\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    293\u001b[0m \u001b[38;5;124;03m        Structured output.\u001b[39;00m\n\u001b[1;32m    294\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 295\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparse\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcompletion\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain_core/output_parsers/pydantic.py:82\u001b[0m, in \u001b[0;36mPydanticOutputParser.parse\u001b[0;34m(self, text)\u001b[0m\n\u001b[1;32m     73\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mparse\u001b[39m(\u001b[38;5;28mself\u001b[39m, text: \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m TBaseModel:\n\u001b[1;32m     74\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Parse the output of an LLM call to a pydantic object.\u001b[39;00m\n\u001b[1;32m     75\u001b[0m \n\u001b[1;32m     76\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     80\u001b[0m \u001b[38;5;124;03m        The parsed pydantic object.\u001b[39;00m\n\u001b[1;32m     81\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 82\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparse\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain_core/output_parsers/json.py:98\u001b[0m, in \u001b[0;36mJsonOutputParser.parse\u001b[0;34m(self, text)\u001b[0m\n\u001b[1;32m     89\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mparse\u001b[39m(\u001b[38;5;28mself\u001b[39m, text: \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m     90\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Parse the output of an LLM call to a JSON object.\u001b[39;00m\n\u001b[1;32m     91\u001b[0m \n\u001b[1;32m     92\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     96\u001b[0m \u001b[38;5;124;03m        The parsed JSON object.\u001b[39;00m\n\u001b[1;32m     97\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 98\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparse_result\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mGeneration\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain_core/output_parsers/pydantic.py:71\u001b[0m, in \u001b[0;36mPydanticOutputParser.parse_result\u001b[0;34m(self, result, partial)\u001b[0m\n\u001b[1;32m     69\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m partial:\n\u001b[1;32m     70\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m---> 71\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m e\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain_core/output_parsers/pydantic.py:66\u001b[0m, in \u001b[0;36mPydanticOutputParser.parse_result\u001b[0;34m(self, result, partial)\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Parse the result of an LLM call to a pydantic object.\u001b[39;00m\n\u001b[1;32m     54\u001b[0m \n\u001b[1;32m     55\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[38;5;124;03m    The parsed pydantic object.\u001b[39;00m\n\u001b[1;32m     64\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     65\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 66\u001b[0m     json_object \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparse_result\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresult\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     67\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_parse_obj(json_object)\n\u001b[1;32m     68\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m OutputParserException \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain_core/output_parsers/json.py:87\u001b[0m, in \u001b[0;36mJsonOutputParser.parse_result\u001b[0;34m(self, result, partial)\u001b[0m\n\u001b[1;32m     85\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m JSONDecodeError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     86\u001b[0m     msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid json output: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtext\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m---> 87\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m OutputParserException(msg, llm_output\u001b[38;5;241m=\u001b[39mtext) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n",
      "\u001b[0;31mOutputParserException\u001b[0m: Invalid json output: {\"Tell me about Sweden.\"}"
     ]
    }
   ],
   "source": [
    "from langchain_core.output_parsers import PydanticOutputParser\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "# 定义输出格式的结构化模型\n",
    "class ResponseFormat(BaseModel):\n",
    "    answer: str = Field(..., description=\"The main answer to the user's query\")\n",
    "    source: str = Field(..., description=\"The source of the answer, such as document ID or URL\")\n",
    "\n",
    "# 创建解析器\n",
    "output_parser = PydanticOutputParser(\n",
    "    pydantic_object=ResponseFormat,\n",
    "    custom_errors={\"ValidationError\": \"输出格式不符合要求，请检查格式！\"}\n",
    ")\n",
    "\n",
    "# 提示模板中加入解析器的格式化指令\n",
    "format_instructions = output_parser.get_format_instructions()\n",
    "print(format_instructions)\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            (\"Answer solely based on the following context:\\n<Documents>\\n{context}\\n</Documents>\"\n",
    "             f\"Format your output as JSON with the following instructions:\\n{format_instructions}\"\n",
    "            )\n",
    "        ),\n",
    "        (\"user\", \"{question}\")\n",
    "    ]\n",
    ")\n",
    "\n",
    "parsed_output = output_parser.parse_with_prompt('{\"Tell me about Sweden.\"}', prompt)\n",
    "print(parsed_output.answer)\n",
    "\n",
    "\n",
    "# # Langchain's LCEL(LangChain Expression Language) Runnable protocol is used to define the chain\n",
    "# # LCEL allows pipe together components and functions\n",
    "# chain = (\n",
    "#     {\"context\": retriever, \"format_instructions\": format_instructions, \"question\": RunnablePassthrough()}\n",
    "#     | prompt\n",
    "#     | llm\n",
    "#     | output_parser\n",
    "# )\n",
    "\n",
    "# try:\n",
    "#     chain.invoke(\"Tell me about Sweden.\")\n",
    "#     # # chain.invoke(\"请向我介绍瑞典。\")\n",
    "#     # # chain.invoke(\"请使用中文向我介绍瑞典的教育。\")\n",
    "#     # chain.invoke(\"请使用中文向我介绍瑞典的文化。\")\n",
    "# except Exception as e:\n",
    "#     print(f\"解析输出时发生错误: {e}\")\n",
    "#     result = ResponseFormat(answer=\"无法生成答案\", source=\"未知\")\n",
    "#     print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c29478b0-0fb1-4678-93cd-b159dc9884a7",
   "metadata": {},
   "source": [
    "## RAG Example with LLM, Embedding & Reranking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "972c310b-5333-4b41-a6dd-ce83e739e6dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"The text does not provide information about Gustav's grandson ascending the throne. The text only provides information up to the establishment of the House of Bernadotte in 1818.\""
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's test a more complex query using the above LLM Embedding chain and see if the reranker can help.\n",
    "chain.invoke(\"In which year Gustav's grandson ascended the throne?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d3854c7-68a3-45b4-9e69-2c4e583d651f",
   "metadata": {},
   "source": [
    "### Enhancing accuracy for single data sources\n",
    "\n",
    "This example demonstrates how a re-ranking model can be used to combine retrieval results and improve accuracy during retrieval of documents.\n",
    "\n",
    "Typically, reranking is a critical piece of high-accuracy, efficient retrieval pipelines. Generally, there are two important use cases:\n",
    "\n",
    "- Combining results from multiple data sources\n",
    "- Enhancing accuracy for single data sources\n",
    "\n",
    "Here, we focus on demonstrating only the second use case. If you want to know more, check [here](https://github.com/langchain-ai/langchain-nvidia/blob/main/libs/ai-endpoints/docs/retrievers/nvidia_rerank.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b7e8677e-a37f-42e2-8fea-4c4413f7d682",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Gustav's grandson, Sigismund, ascended the throne in 1592.\""
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_nvidia_ai_endpoints import NVIDIARerank\n",
    "from langchain_core.runnables import RunnableParallel\n",
    "\n",
    "# We will narrow the collection to 100 results and further narrow it to 10 with the reranker.\n",
    "retriever = store.as_retriever(search_kwargs={'k':100}) # typically k will be 1000 for real world use-cases\n",
    "ranker = NVIDIARerank(model='nv-rerank-qa-mistral-4b:1', top_n=10)\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"Answer solely based on the following context:\\n<Documents>\\n{context}\\n</Documents>\",\n",
    "        ),\n",
    "        (\"user\", \"{question}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "reranker = lambda input: ranker.compress_documents(query=input['question'], documents=input['context'])\n",
    "\n",
    "chain_with_ranker = (\n",
    "    RunnableParallel({\"context\": retriever, \"question\": RunnablePassthrough()})\n",
    "    | {\"context\": reranker, \"question\": lambda input: input['question']}\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "chain_with_ranker.invoke(\"In which year Gustav's grandson ascended the throne?\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79d5186f-12c0-47c9-9e85-5987fedf7b97",
   "metadata": {},
   "source": [
    "#### Note:\n",
    "- In this notebook, we have used NVIDIA NIM microservices from the NVIDIA API Catalog.\n",
    "- The above APIs, ChatNVIDIA, NVIDIAEmbedding, and NVIDIARerank, also support self-hosted NIM microservices.\n",
    "- Change the `base_url` to your deployed NIM URL.\n",
    "- Example: `llm = ChatNVIDIA(base_url=\"http://localhost:8000/v1\", model=\"meta/llama3-8b-instruct\")`\n",
    "- NIM can be hosted locally using Docker, following the [NVIDIA NIM for LLMs](https://docs.nvidia.com/nim/large-language-models/latest/getting-started.html) documentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "61236a22-922f-403f-89d1-1172251aeb0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example Code snippet if you want to use a self-hosted NIM\n",
    "from langchain_nvidia_ai_endpoints import ChatNVIDIA\n",
    "\n",
    "# connect to an LLM NIM running at localhost:8000, specifying a specific model\n",
    "llm = ChatNVIDIA(base_url=\"http://localhost:8000/v1\", model=\"meta/llama3-8b-instruct\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
